{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe0f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.patches import Rectangle\n",
    "from pathlib import Path\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from utils import load_pickle,extract_used_data,set_params\n",
    "from utils.config import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e38f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(entry_dir: str):\n",
    "    path = Path(entry_dir)\n",
    "    data_dict = {}\n",
    "    for file in path.glob(\"*.pkl\"):\n",
    "        data = load_pickle(file)\n",
    "        data = extract_used_data(data)\n",
    "        data_dict[file.stem] = data\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def extract_ab_ba(data: dict, params: Params, ana_tt, ana_bt):\n",
    "\n",
    "    data_AB = []\n",
    "    data_BA = []\n",
    "\n",
    "    for index in params.ana_index_grid(ana_tt, ana_bt):\n",
    "\n",
    "        trial_type = params.tt[index[0]]\n",
    "        fr = data[\"simple_firing\"][index]\n",
    "        if fr is None:\n",
    "            continue\n",
    "\n",
    "        fr = gaussian_filter1d(fr, sigma=params.gaussian_sigma, axis=2, mode=\"nearest\", truncate=3.0)\n",
    "        k = params.len_pos_average\n",
    "        len_track = fr.shape[2]\n",
    "        fr = fr[:,:,:(len_track - len_track % k)]\n",
    "        fr = resample_poly(fr, up=1, down=k, axis=2)\n",
    "        \n",
    "        if \"CAB\" in trial_type or \"ACB\" in trial_type or \"ABC\" in trial_type:\n",
    "            data_AB.append(fr)\n",
    "        elif \"CBA\" in trial_type or \"BCA\" in trial_type or \"BAC\" in trial_type:\n",
    "            data_BA.append(fr)\n",
    "        else:\n",
    "            raise ValueError(\"Trial type not recognized:\", trial_type)\n",
    "\n",
    "    data_AB = np.concatenate(data_AB, axis=1)\n",
    "    data_BA = np.concatenate(data_BA, axis=1)\n",
    "\n",
    "    return data_AB, data_BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbf35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_cb = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_cb\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a1897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _task_score_decode(task, X_all, labels, perm_labels, splits, C, max_iter):\n",
    "    \"\"\"\n",
    "    task = (shuffle_idx, fold_idx, t)\n",
    "      shuffle_idx = -1: real labels\n",
    "      shuffle_idx >=0: perm_labels[shuffle_idx]\n",
    "    \"\"\"\n",
    "    shuffle_idx, fold_idx, t = task\n",
    "\n",
    "    y = labels if shuffle_idx < 0 else perm_labels[shuffle_idx]\n",
    "\n",
    "    train_idx, test_idx = splits[fold_idx]\n",
    "\n",
    "    X_t = X_all[:, :, t].T  # (n_trials, n_neurons)\n",
    "    X_train, X_test = X_t[train_idx], X_t[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm\", LinearSVC(C=C, max_iter=max_iter))\n",
    "    ])\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    sc = float(clf.score(X_test, y_test))\n",
    "    return (shuffle_idx, fold_idx, t, sc)\n",
    "\n",
    "\n",
    "def decode_and_shuffle(\n",
    "    data_AB, data_BA,\n",
    "    n_splits=5, n_repeats=10,\n",
    "    n_shuffles=500,\n",
    "    n_jobs=12,\n",
    "    random_state=0,\n",
    "    backend=\"threading\",\n",
    "    C=1.0,\n",
    "    max_iter=20000,\n",
    "):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    neurons, n_AB, n_time = data_AB.shape\n",
    "    _, n_BA, _ = data_BA.shape\n",
    "\n",
    "    X_all = np.concatenate([data_AB, data_BA], axis=1)  # (neurons, n_trials, n_time)\n",
    "    labels = np.concatenate([\n",
    "        np.zeros(n_AB, dtype=int),\n",
    "        np.ones(n_BA, dtype=int)\n",
    "    ])  # (n_trials,)\n",
    "\n",
    "    n_trials = labels.size\n",
    "\n",
    "    # repeated stratified CV (keeps class ratios)\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    splits = list(cv.split(np.zeros(n_trials), labels))\n",
    "    n_folds = len(splits)\n",
    "\n",
    "    # pre-generate permuted labels\n",
    "    perm_labels = np.empty((n_shuffles, n_trials), dtype=int)\n",
    "    for i in range(n_shuffles):\n",
    "        perm_labels[i] = rng.permutation(labels)\n",
    "\n",
    "    # build tasks: real + shuffles\n",
    "    tasks = []\n",
    "    # real (shuffle_idx=-1)\n",
    "    for fold_idx in range(n_folds):\n",
    "        for t in range(n_time):\n",
    "            tasks.append((-1, fold_idx, t))\n",
    "    # shuffle\n",
    "    for i_shuf in range(n_shuffles):\n",
    "        for fold_idx in range(n_folds):\n",
    "            for t in range(n_time):\n",
    "                tasks.append((i_shuf, fold_idx, t))\n",
    "\n",
    "    total_tasks = len(tasks)\n",
    "\n",
    "    pbar = tqdm(total=total_tasks, desc=\"Shuffle & Decode\", dynamic_ncols=True)\n",
    "    with tqdm_joblib(pbar):\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend)(\n",
    "            delayed(_task_score_decode)(task, X_all, labels, perm_labels, splits, C, max_iter)\n",
    "            for task in tasks\n",
    "        )\n",
    "\n",
    "    # aggregate\n",
    "    scores_real = np.zeros((n_folds, n_time), dtype=float)\n",
    "    scores_shuffle = np.zeros((n_shuffles, n_folds, n_time), dtype=float)\n",
    "\n",
    "    for shuf_idx, fold_idx, t, sc in results:\n",
    "        if shuf_idx < 0:\n",
    "            scores_real[fold_idx, t] = sc\n",
    "        else:\n",
    "            scores_shuffle[shuf_idx, fold_idx, t] = sc\n",
    "\n",
    "    mean_acc = np.mean(scores_real, axis=0)\n",
    "    sem_acc  = np.std(scores_real, axis=0, ddof=1) / np.sqrt(n_folds)\n",
    "\n",
    "    # shuffle: take mean across folds for each shuffle\n",
    "    shuffle_means = np.mean(scores_shuffle, axis=1)  # (n_shuffles, n_time)\n",
    "    shuffle_lower = np.percentile(shuffle_means, 2.5, axis=0)\n",
    "    shuffle_upper = np.percentile(shuffle_means, 97.5, axis=0)\n",
    "\n",
    "    # cluster stats (1D over time): same as your original\n",
    "    sig_mask = mean_acc > shuffle_upper\n",
    "    clusters = []\n",
    "    current = None\n",
    "    for i, v in enumerate(sig_mask):\n",
    "        if v:\n",
    "            if current is None:\n",
    "                current = [i, i]\n",
    "            else:\n",
    "                current[1] = i\n",
    "        else:\n",
    "            if current is not None:\n",
    "                clusters.append(tuple(current))\n",
    "                current = None\n",
    "    if current:\n",
    "        clusters.append(tuple(current))\n",
    "\n",
    "    real_cluster_stats = []\n",
    "    for start_idx, end_idx in clusters:\n",
    "        stat = np.sum(mean_acc[start_idx:end_idx+1] - shuffle_upper[start_idx:end_idx+1])\n",
    "        real_cluster_stats.append((start_idx, end_idx, stat))\n",
    "\n",
    "    null_cluster_stats = []\n",
    "    for i in range(n_shuffles):\n",
    "        perm = shuffle_means[i]\n",
    "        perm_mask = perm > shuffle_upper\n",
    "        c = None\n",
    "        for j, v in enumerate(perm_mask):\n",
    "            if v:\n",
    "                if c is None:\n",
    "                    c = [j, j]\n",
    "                else:\n",
    "                    c[1] = j\n",
    "            else:\n",
    "                if c is not None:\n",
    "                    mass = np.sum(perm[c[0]:c[1]+1] - shuffle_upper[c[0]:c[1]+1])\n",
    "                    null_cluster_stats.append(mass)\n",
    "                    c = None\n",
    "        if c:\n",
    "            mass = np.sum(perm[c[0]:c[1]+1] - shuffle_upper[c[0]:c[1]+1])\n",
    "            null_cluster_stats.append(mass)\n",
    "\n",
    "    null_cluster_stats = np.array(null_cluster_stats, dtype=float)\n",
    "\n",
    "    cluster_signif = []\n",
    "    for start_idx, end_idx, stat in real_cluster_stats:\n",
    "        pval = (np.sum(null_cluster_stats >= stat) + 1) / (len(null_cluster_stats) + 1)\n",
    "        cluster_signif.append((start_idx, end_idx, pval))\n",
    "\n",
    "    return mean_acc, sem_acc, shuffle_lower, shuffle_upper, cluster_signif\n",
    "\n",
    "\n",
    "def _task_tgm_chunk(task, X, y, splits, time_idx, C, max_iter):\n",
    "    \"\"\"\n",
    "    task = (fold_idx, chunk_start, chunk_end)\n",
    "    X: (n_trials, n_neurons, n_time)\n",
    "    time_idx: (T,) list of used time indices\n",
    "    return: (fold_idx, chunk_start, rows) rows shape (chunk_len, T)\n",
    "    \"\"\"\n",
    "    fold_idx, cs, ce = task\n",
    "    train_idx, test_idx = splits[fold_idx]\n",
    "\n",
    "    y_train = y[train_idx]\n",
    "    y_test  = y[test_idx]\n",
    "    T = len(time_idx)\n",
    "\n",
    "    # test data for all t_test once: (T, n_test, n_neurons)\n",
    "    X_test_all = X[test_idx][:, :, time_idx]          # (n_test, n_neurons, T)\n",
    "    X_test_all = np.transpose(X_test_all, (2, 0, 1))  # (T, n_test, n_neurons)\n",
    "\n",
    "    rows = np.empty((ce - cs, T), dtype=float)\n",
    "\n",
    "    for ii, t_train in enumerate(time_idx[cs:ce]):\n",
    "        # TRAIN at one time\n",
    "        X_train = X[train_idx, :, t_train]  # (n_train, n_neurons)\n",
    "\n",
    "        # fit scaler ONLY on training to avoid leakage :contentReference[oaicite:5]{index=5}\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        X_train_z = scaler.fit_transform(X_train)\n",
    "\n",
    "        mean = scaler.mean_\n",
    "        scale = scaler.scale_.copy()\n",
    "        scale[scale == 0] = 1.0\n",
    "\n",
    "        clf = LinearSVC(C=C, max_iter=max_iter)\n",
    "        clf.fit(X_train_z, y_train)\n",
    "\n",
    "        w = clf.coef_.ravel()\n",
    "        b = float(clf.intercept_[0])\n",
    "\n",
    "        # standardize all test times in one shot: (T, n_test, n_neurons)\n",
    "        Xz = (X_test_all - mean[None, None, :]) / scale[None, None, :]\n",
    "\n",
    "        # decision scores: (T, n_test)\n",
    "        scores = np.tensordot(Xz, w, axes=([2], [0])) + b\n",
    "\n",
    "        pred = (scores > 0).astype(int)\n",
    "        acc  = (pred == y_test[None, :]).mean(axis=1)  # (T,)\n",
    "        rows[ii, :] = acc\n",
    "\n",
    "    return fold_idx, cs, rows\n",
    "\n",
    "\n",
    "def temporal_generalization(\n",
    "    data_AB, data_BA,\n",
    "    n_splits=5, n_repeats=10,\n",
    "    n_jobs=12,\n",
    "    random_state=0,\n",
    "    backend=\"threading\",\n",
    "    C=1.0,\n",
    "    max_iter=20000,\n",
    "    chunk_size=10,\n",
    "):\n",
    "    data_AB = np.asarray(data_AB, dtype=float)\n",
    "    data_BA = np.asarray(data_BA, dtype=float)\n",
    "    n_neurons, n_AB, n_time = data_AB.shape\n",
    "    _, n_BA, n_time2 = data_BA.shape\n",
    "    if n_time2 != n_time:\n",
    "        raise ValueError(\"data_AB and data_BA must have same n_time\")\n",
    "\n",
    "    X_all = np.concatenate([data_AB, data_BA], axis=1)  # (n_neurons, n_trials, n_time)\n",
    "    y = np.concatenate([np.zeros(n_AB, int), np.ones(n_BA, int)])\n",
    "    n_trials = y.size\n",
    "\n",
    "    # reshape to (n_trials, n_neurons, n_time)\n",
    "    X = X_all.transpose(1, 0, 2)\n",
    "\n",
    "    time_idx = np.arange(0, n_time, dtype=int)\n",
    "    T = len(time_idx)\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    splits = list(cv.split(np.zeros(n_trials), y))\n",
    "    n_folds = len(splits)\n",
    "\n",
    "    # tasks: (fold, chunk_start, chunk_end)\n",
    "    tasks = []\n",
    "    for fold_idx in range(n_folds):\n",
    "        for cs in range(0, T, chunk_size):\n",
    "            ce = min(cs + chunk_size, T)\n",
    "            tasks.append((fold_idx, cs, ce))\n",
    "\n",
    "    pbar = tqdm(total=len(tasks), desc=\"TGM\", dynamic_ncols=True)\n",
    "    with tqdm_joblib(pbar):\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend)(\n",
    "            delayed(_task_tgm_chunk)(task, X, y, splits, time_idx, C, max_iter)\n",
    "            for task in tasks\n",
    "        )\n",
    "\n",
    "    # aggregate across folds using sum & sumsq\n",
    "    sum_mat = np.zeros((T, T), dtype=float)\n",
    "    sumsq_mat = np.zeros((T, T), dtype=float)\n",
    "\n",
    "    for fold_idx, cs, rows in results:\n",
    "        ce = cs + rows.shape[0]\n",
    "        sum_mat[cs:ce, :] += rows\n",
    "        sumsq_mat[cs:ce, :] += rows * rows\n",
    "\n",
    "    tgm_mean = sum_mat / n_folds\n",
    "\n",
    "    if n_folds > 1:\n",
    "        var = (sumsq_mat - (sum_mat * sum_mat) / n_folds) / (n_folds - 1)\n",
    "        var = np.maximum(var, 0.0)\n",
    "        tgm_sem = np.sqrt(var / n_folds)\n",
    "    else:\n",
    "        tgm_sem = np.zeros_like(tgm_mean)\n",
    "\n",
    "    return tgm_mean, tgm_sem\n",
    "\n",
    "\n",
    "# mean_acc, sem_acc, shuf_lo, shuf_hi, clusters = decode_and_shuffle(\n",
    "#     data_AB, data_BA,\n",
    "#     n_splits=5, n_repeats=10,\n",
    "#     n_shuffles=100,\n",
    "#     n_jobs=12,\n",
    "#     backend=\"threading\",   # or \"loky\"\n",
    "#     random_state=0\n",
    "# )\n",
    "#\n",
    "# tgm_mean, tgm_sem, time_idx = temporal_generalization_no_perm(\n",
    "#     data_AB, data_BA,\n",
    "#     n_splits=5, n_repeats=10,\n",
    "#     n_jobs=12,\n",
    "#     backend=\"threading\",\n",
    "#     time_stride=1,\n",
    "#     chunk_size=10\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a74d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decoding_with_zones(data: dict, params: Params, mean_acc, sem_acc, shuffle_lower, shuffle_upper, cluster_signif):\n",
    "\n",
    "    n_time = len(mean_acc)\n",
    "    X = np.arange(n_time)\n",
    "\n",
    "    zones_color = {\n",
    "        1: \"#1f77b4\",\n",
    "        3: \"#1f77b4\",\n",
    "    }\n",
    "    acc_color = \"#2ca02c\"\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # mean + sem\n",
    "    ax.plot(X, mean_acc, color=acc_color)\n",
    "    ax.fill_between(X, mean_acc - sem_acc, mean_acc + sem_acc, color=acc_color, alpha=0.4)\n",
    "\n",
    "    # shuffle band\n",
    "    ax.fill_between(X, shuffle_lower, shuffle_upper, color=\"gray\", alpha=0.4)\n",
    "\n",
    "    # chance line\n",
    "    ax.axhline(0.5, linestyle=\"--\", color=\"k\")\n",
    "\n",
    "    # cluster significance highlight\n",
    "    for start, end, p in cluster_signif:\n",
    "        if p < 0.05:\n",
    "            ax.axvspan(start, end, facecolor=\"black\", alpha=0.2)\n",
    "\n",
    "    # zones background (horizontal bars)\n",
    "    x0, x1 = ax.get_xlim()\n",
    "    eps = 0.002 * (x1 - x0)\n",
    "    h_frac = 0.05\n",
    "\n",
    "    zones = data[\"zones\"]\n",
    "\n",
    "    for zi, row in enumerate(zones):\n",
    "\n",
    "        if zi not in zones_color: continue\n",
    "\n",
    "        start_x = row[0] / params.space_unit / params.len_pos_average\n",
    "        end_x   = row[1]   / params.space_unit / params.len_pos_average\n",
    "\n",
    "        ax.axvline(start_x, linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "        ax.axvline(end_x,   linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "\n",
    "        color_zone = zones_color[zi]\n",
    "\n",
    "        # rect = Rectangle(\n",
    "        #     (start_x - eps, 1.0 - h_frac),\n",
    "        #     end_x - start_x + 2*eps, h_frac,\n",
    "        #     transform=ax.get_xaxis_transform(),\n",
    "        #     facecolor=color_zone,\n",
    "        #     edgecolor=\"none\",\n",
    "        #     clip_on=False,\n",
    "        #     zorder=6,\n",
    "        # )\n",
    "        # ax.add_patch(rect)\n",
    "\n",
    "    # aesthetics\n",
    "    ax.set_ylim(0, 1.02)\n",
    "    ax.set_xlabel(\"Position (cm)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        mticker.FuncFormatter(lambda x, pos: f\"{x*params.space_unit*params.len_pos_average:g}\")\n",
    "    )\n",
    "    ax.tick_params(labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed976dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tgm_heatmap(\n",
    "    data: dict,\n",
    "    params: Params,\n",
    "    tgm_mean: np.ndarray,\n",
    "    *,\n",
    "    vmin: float | None = None,\n",
    "    vmax: float | None = None,\n",
    "    show_diag: bool = True,\n",
    "    show_zones: bool = True,\n",
    "):\n",
    "\n",
    "    tgm_mean = np.asarray(tgm_mean, dtype=float)\n",
    "    if tgm_mean.ndim != 2 or tgm_mean.shape[0] != tgm_mean.shape[1]:\n",
    "        raise ValueError(\"tgm_mean must be a square 2D array (T, T)\")\n",
    "    T = tgm_mean.shape[0]\n",
    "\n",
    "    time_idx = np.arange(T, dtype=float)\n",
    "\n",
    "    bin_cm = float(params.space_unit * params.len_pos_average)\n",
    "    coord = time_idx * bin_cm\n",
    "    xlabel = \"Test position (cm)\"\n",
    "    ylabel = \"Train position (cm)\"\n",
    "    tick_formatter = mticker.FuncFormatter(lambda x, pos: f\"{x:g}\")\n",
    "\n",
    "    if T > 1:\n",
    "        d = float(coord[1] - coord[0])\n",
    "    else:\n",
    "        d = 1.0\n",
    "    extent = [coord[0] - d/2, coord[-1] + d/2, coord[0] - d/2, coord[-1] + d/2]\n",
    "\n",
    "    chance = 0.5\n",
    "    if vmin is None or vmax is None:\n",
    "        dev = np.nanmax(np.abs(tgm_mean - chance))\n",
    "        if not np.isfinite(dev) or dev == 0:\n",
    "            dev = 0.05\n",
    "        if vmin is None:\n",
    "            vmin = chance - dev\n",
    "        if vmax is None:\n",
    "            vmax = chance + dev\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    ax = fig.gca()\n",
    "    title = \"Temporal generalization\"\n",
    "    cbar_labels = \"Score\"\n",
    "\n",
    "    cmap = \"viridis\"\n",
    "    im = ax.imshow(\n",
    "        tgm_mean,\n",
    "        origin=\"lower\",\n",
    "        extent=extent,\n",
    "        aspect=\"auto\",\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    ax.yaxis.set_ticks_position(\"left\")\n",
    "    if tick_formatter is not None:\n",
    "        ax.xaxis.set_major_formatter(tick_formatter)\n",
    "        ax.yaxis.set_major_formatter(tick_formatter)\n",
    "\n",
    "    if show_diag:\n",
    "        ax.plot([coord[0], coord[-1]], [coord[0], coord[-1]], linestyle=\"--\", linewidth=0.8, color=\"k\", alpha=0.6)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(cbar_labels, fontsize=11)\n",
    "\n",
    "    zones = data[\"zones\"]\n",
    "    zones_color = {\n",
    "        1: \"#1f77b4\",\n",
    "        3: \"#1f77b4\",\n",
    "    }\n",
    "\n",
    "    if show_zones and zones is not None and len(zones) > 0 and params is not None:\n",
    "\n",
    "        for zi, row in enumerate(zones):\n",
    "\n",
    "            if zi not in zones_color: continue\n",
    "\n",
    "            start_x = row[0]\n",
    "            end_x   = row[1]\n",
    "\n",
    "            ax.axvline(start_x, linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "            ax.axvline(end_x,   linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "            ax.axhline(start_x, linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "            ax.axhline(end_x,   linestyle=\"--\", color=\"k\", linewidth=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060b95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on(entry_dir, params, ana_tt, ana_bt):\n",
    "    \n",
    "    data_dict = get_data_dict(entry_dir)\n",
    "\n",
    "    for key, data in data_dict.items():\n",
    "        if key != \"RDH01-PFCsep2\" and key != \"RDP02-PFCsep\":\n",
    "            continue\n",
    "        print(\"===============\")\n",
    "        print(key)\n",
    "        print(\"===============\")\n",
    "        data_AB, data_BA = extract_ab_ba(data, params, ana_tt, ana_bt)\n",
    "        mean_acc, sem_acc, shuf_lo, shuf_hi, clusters = decode_and_shuffle(data_AB, data_BA, backend=\"loky\")\n",
    "        plot_decoding_with_zones(data, params, mean_acc, sem_acc, shuf_lo, shuf_hi, clusters)\n",
    "        tgm_mean, _ = temporal_generalization(data_AB, data_BA)\n",
    "        plot_tgm_heatmap(data, params, tgm_mean, vmin=0.2, vmax=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a02ca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "RDH01-PFCsep2\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124a21f2c79d44969b3d8b09873b4133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle & Decode:   0%|          | 0/7965900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m ana_tt\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m ana_bt\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m run_on(entry_dir, params, ana_tt, ana_bt)\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mrun_on\u001b[1;34m(entry_dir, params, ana_tt, ana_bt)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===============\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m data_AB, data_BA \u001b[38;5;241m=\u001b[39m extract_ab_ba(data, params, ana_tt, ana_bt)\n\u001b[1;32m---> 12\u001b[0m mean_acc, sem_acc, shuf_lo, shuf_hi, clusters \u001b[38;5;241m=\u001b[39m decode_and_shuffle(data_AB, data_BA, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m plot_decoding_with_zones(data, params, mean_acc, sem_acc, shuf_lo, shuf_hi, clusters)\n\u001b[0;32m     14\u001b[0m tgm_mean, _ \u001b[38;5;241m=\u001b[39m temporal_generalization(data_AB, data_BA)\n",
      "Cell \u001b[1;32mIn[4], line 80\u001b[0m, in \u001b[0;36mdecode_and_shuffle\u001b[1;34m(data_AB, data_BA, n_splits, n_repeats, n_shuffles, n_jobs, random_state, backend, C, max_iter)\u001b[0m\n\u001b[0;32m     78\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_tasks, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShuffle & Decode\u001b[39m\u001b[38;5;124m\"\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm_joblib(pbar):\n\u001b[1;32m---> 80\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, backend\u001b[38;5;241m=\u001b[39mbackend)(\n\u001b[0;32m     81\u001b[0m         delayed(_task_score_decode)(task, X_all, labels, perm_labels, splits, C, max_iter)\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# aggregate\u001b[39;00m\n\u001b[0;32m     86\u001b[0m scores_real \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n_folds, n_time), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\neuro\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\neuro\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\neuro\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entry_dir = \"../../data/flexible_shift/\"\n",
    "\n",
    "params = set_params(tt_preset=\"basic\",\n",
    "                    bt_preset=\"basic\",\n",
    "                    len_pos_average=10,\n",
    "                    gaussian_sigma=50)\n",
    "\n",
    "ana_tt=[\"*\"]\n",
    "ana_bt=[\"*\"]\n",
    "\n",
    "run_on(entry_dir, params, ana_tt, ana_bt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
