{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71998065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import resample_poly\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from utils import load_pickle,extract_used_data,set_params\n",
    "from utils.config import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2972442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_cb = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_cb\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def get_data(entry_dir: str):\n",
    "    path = Path(entry_dir)\n",
    "    data_list = []\n",
    "    for file in path.glob(\"*.pkl\"):\n",
    "        data = load_pickle(file)\n",
    "        data = extract_used_data(data)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def _clamp_slice(s: int, e: int, T: int) -> tuple[int, int]:\n",
    "    s = int(s); e = int(e)\n",
    "    s = max(0, min(s, T))\n",
    "    e = max(0, min(e, T))\n",
    "    return s, e\n",
    "\n",
    "\n",
    "def extract_ab_ba_one_session(\n",
    "    data: dict,\n",
    "    params: Params,\n",
    "    ana_tt: list,\n",
    "    ana_bt: list,\n",
    "    *,\n",
    "    extend_cm: float = 10.0,\n",
    "    firing_key: str = \"simple_firing\",\n",
    "):\n",
    "    zones = data[\"zones\"]\n",
    "    zone1_idx: int = 1,\n",
    "    zone2_idx: int = 3,\n",
    "\n",
    "    denom = (params.space_unit * params.len_pos_average)\n",
    "    s1 = int(np.floor((zones[zone1_idx][0] - extend_cm) / denom))\n",
    "    e1 = int(np.ceil ((zones[zone1_idx][1] + extend_cm) / denom))\n",
    "    s2 = int(np.floor((zones[zone2_idx][0] - extend_cm) / denom))\n",
    "    e2 = int(np.ceil ((zones[zone2_idx][1] + extend_cm) / denom))\n",
    "\n",
    "    data_AB_list = []\n",
    "    data_BA_list = []\n",
    "\n",
    "    for index in params.ana_index_grid(ana_tt, ana_bt):\n",
    "        trial_type = params.tt[index[0]]\n",
    "        fr = data[firing_key][index]\n",
    "        if fr is None:\n",
    "            continue\n",
    "\n",
    "        # fr: (neurons, trials, position)\n",
    "        fr = gaussian_filter1d(fr, sigma=params.gaussian_sigma, axis=2, mode=\"nearest\", truncate=3.0)\n",
    "\n",
    "        # downsample along position axis\n",
    "        k = params.len_pos_average\n",
    "        len_track = fr.shape[2]\n",
    "        fr = fr[:, :, :(len_track - len_track % k)]\n",
    "        fr = resample_poly(fr, up=1, down=k, axis=2)  # (neurons, trials, position_ds)\n",
    "\n",
    "        T = fr.shape[2]\n",
    "        s1c, e1c = _clamp_slice(s1, e1, T)\n",
    "        s2c, e2c = _clamp_slice(s2, e2, T)\n",
    "        if (e1c <= s1c) or (e2c <= s2c):\n",
    "            continue\n",
    "\n",
    "        seg1 = fr[:, :, s1c:e1c]\n",
    "        seg2 = fr[:, :, s2c:e2c]\n",
    "        fr_seg = np.concatenate([seg1, seg2], axis=2)  # (neurons, trials, L)\n",
    "\n",
    "        if (\"CAB\" in trial_type) or (\"ACB\" in trial_type) or (\"ABC\" in trial_type):\n",
    "            data_AB_list.append(fr_seg)\n",
    "        elif (\"CBA\" in trial_type) or (\"BCA\" in trial_type) or (\"BAC\" in trial_type):\n",
    "            data_BA_list.append(fr_seg)\n",
    "        else:\n",
    "            raise ValueError(f\"Trial type not recognized: {trial_type}\")\n",
    "\n",
    "    if (len(data_AB_list) == 0) or (len(data_BA_list) == 0):\n",
    "        return None, None\n",
    "\n",
    "    data_AB = np.concatenate(data_AB_list, axis=1)  # (neurons, nAB, L)\n",
    "    data_BA = np.concatenate(data_BA_list, axis=1)  # (neurons, nBA, L)\n",
    "    return data_AB, data_BA\n",
    "\n",
    "\n",
    "\n",
    "def _compress_window_to_scalar(data_AB, data_BA, *, reduce=\"mean\"):\n",
    "    if reduce == \"mean\":\n",
    "        x_ab = data_AB.mean(axis=2)\n",
    "        x_ba = data_BA.mean(axis=2)\n",
    "    elif reduce == \"sum\":\n",
    "        x_ab = data_AB.sum(axis=2)\n",
    "        x_ba = data_BA.sum(axis=2)\n",
    "    else:\n",
    "        raise ValueError(\"reduce must be 'mean' or 'sum'\")\n",
    "\n",
    "    X = np.concatenate([x_ab, x_ba], axis=1)  # (neurons, n_trials)\n",
    "    y = np.concatenate([\n",
    "        np.zeros(x_ab.shape[1], dtype=int),\n",
    "        np.ones(x_ba.shape[1], dtype=int),\n",
    "    ])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AUC / d' + permutation + FDR\n",
    "# =========================\n",
    "def _dprime_from_sums(pos_sum, pos_sum_sq, total_sum, total_sum_sq, n_pos, n_neg, eps=1e-12):\n",
    "    neg_sum = total_sum - pos_sum\n",
    "    neg_sum_sq = total_sum_sq - pos_sum_sq\n",
    "\n",
    "    pos_mean = pos_sum / n_pos\n",
    "    neg_mean = neg_sum / n_neg\n",
    "\n",
    "    # sample var (ddof=1): (sum_sq - sum^2/n)/(n-1)\n",
    "    pos_var = (pos_sum_sq - (pos_sum * pos_sum) / n_pos) / max(n_pos - 1, 1)\n",
    "    neg_var = (neg_sum_sq - (neg_sum * neg_sum) / n_neg) / max(n_neg - 1, 1)\n",
    "\n",
    "    pos_var = np.maximum(pos_var, 0.0)\n",
    "    neg_var = np.maximum(neg_var, 0.0)\n",
    "\n",
    "    dp = (pos_mean - neg_mean) / np.sqrt(0.5 * (pos_var + neg_var) + eps)\n",
    "    return dp\n",
    "\n",
    "\n",
    "def _one_neuron_scalar_perm(x, y, M, n_pos, n_neg, test_stat, eps=1e-12):\n",
    "    \"\"\"\n",
    "    x: (n_trials,)\n",
    "    y: (n_trials,) 0/1\n",
    "    M: (n_perm, n_trials) float32, 1 表示 perm 标签=1\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float64, copy=False)\n",
    "\n",
    "    if test_stat == \"auc\":\n",
    "        # rank-sum AUC，避免每次 perm 都调用 roc_auc_score（更快）\n",
    "        r = rankdata(x, method=\"average\").astype(np.float64)\n",
    "        pos_rank_sum_obs = r[y == 1].sum()\n",
    "        auc_obs = (pos_rank_sum_obs - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n",
    "        stat_obs = abs(auc_obs - 0.5)\n",
    "\n",
    "        pos_rank_sum_perm = M @ r\n",
    "        auc_perm = (pos_rank_sum_perm - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n",
    "        stat_null = np.abs(auc_perm - 0.5)\n",
    "\n",
    "        p = (1.0 + np.sum(stat_null >= stat_obs)) / (len(stat_null) + 1.0)\n",
    "        return auc_obs, stat_obs, p\n",
    "\n",
    "    elif test_stat == \"dprime\":\n",
    "        x2 = x * x\n",
    "        total_sum = x.sum()\n",
    "        total_sum_sq = x2.sum()\n",
    "\n",
    "        pos_sum_obs = x[y == 1].sum()\n",
    "        pos_sum_sq_obs = x2[y == 1].sum()\n",
    "        dp_obs = _dprime_from_sums(pos_sum_obs, pos_sum_sq_obs, total_sum, total_sum_sq, n_pos, n_neg, eps=eps)\n",
    "        stat_obs = abs(dp_obs)\n",
    "\n",
    "        pos_sum_perm = M @ x\n",
    "        pos_sum_sq_perm = M @ x2\n",
    "        dp_perm = _dprime_from_sums(pos_sum_perm, pos_sum_sq_perm, total_sum, total_sum_sq, n_pos, n_neg, eps=eps)\n",
    "        stat_null = np.abs(dp_perm)\n",
    "\n",
    "        p = (1.0 + np.sum(stat_null >= stat_obs)) / (len(stat_null) + 1.0)\n",
    "        return dp_obs, stat_obs, p\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"test_stat must be 'auc' or 'dprime'\")\n",
    "\n",
    "\n",
    "def count_sig_neurons_auc_dprime(\n",
    "    data_list,\n",
    "    params: Params,\n",
    "    ana_tt,\n",
    "    ana_bt,\n",
    "    *,\n",
    "    extend_cm=10.0,\n",
    "    reduce=\"mean\",\n",
    "    n_perm=5000,\n",
    "    alpha=0.05,\n",
    "    seed=0,\n",
    "    test_stat=\"auc\",        # \"auc\" or \"dprime\"\n",
    "    n_jobs=8,\n",
    "    backend=\"threading\",    # threading(多线程) or \"loky\"(多进程)\n",
    "):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    out = []\n",
    "\n",
    "    for si, data in enumerate(tqdm(data_list, desc=\"sessions\", dynamic_ncols=True)):\n",
    "        data_AB, data_BA = extract_ab_ba_one_session(data, params, ana_tt, ana_bt, extend_cm=extend_cm)\n",
    "        if data_AB is None:\n",
    "            out.append({\"session\": si, \"n_sig\": 0, \"n_neuron\": 0, \"p\": None, \"q\": None})\n",
    "            continue\n",
    "\n",
    "        X, y = _compress_window_to_scalar(data_AB, data_BA, reduce=reduce)  # (neurons, n_trials)\n",
    "        n_neuron, n_trials = X.shape\n",
    "        n_pos = int(y.sum())\n",
    "        n_neg = int(n_trials - n_pos)\n",
    "        if n_pos < 2 or n_neg < 2:\n",
    "            out.append({\"session\": si, \"n_sig\": 0, \"n_neuron\": n_neuron, \"p\": None, \"q\": None})\n",
    "            continue\n",
    "\n",
    "        perm_idx = np.array([rng.permutation(n_trials) for _ in range(n_perm)], dtype=np.int32)\n",
    "        y_perm = y[perm_idx]\n",
    "        M = (y_perm == 1).astype(np.float32)  # (n_perm, n_trials)\n",
    "\n",
    "        with tqdm_joblib(tqdm(total=n_neuron, desc=f\"session {si} neurons ({test_stat})\",\n",
    "                              leave=False, dynamic_ncols=True)):\n",
    "            results = Parallel(n_jobs=n_jobs, backend=backend)(\n",
    "                delayed(_one_neuron_scalar_perm)(X[n, :], y, M, n_pos, n_neg, test_stat)\n",
    "                for n in range(n_neuron)\n",
    "            )\n",
    "\n",
    "        obs_val = np.array([r[0] for r in results], dtype=float)   # auc 或 dp\n",
    "        stat_obs = np.array([r[1] for r in results], dtype=float)  # abs(auc-0.5) 或 abs(dp)\n",
    "        pvals = np.array([r[2] for r in results], dtype=float)\n",
    "\n",
    "        rej, qvals, _, _ = multipletests(pvals, alpha=alpha, method=\"fdr_bh\")\n",
    "        out.append({\n",
    "            \"session\": si,\n",
    "            \"n_sig\": int(rej.sum()),\n",
    "            \"n_neuron\": n_neuron,\n",
    "            \"p\": pvals,\n",
    "            \"q\": qvals,\n",
    "            \"obs\": obs_val,\n",
    "            \"stat\": stat_obs,\n",
    "            \"sig_mask\": rej,\n",
    "            \"test_stat\": test_stat,\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Fisher-vector + permutation + FDR\n",
    "#    - cov=\"diag\": 超快（利用时间维作为向量，但忽略协方差）\n",
    "#    - cov=\"lw\":   LedoitWolf shrinkage（利用协方差，慢，建议 time_bin）\n",
    "# =========================\n",
    "\n",
    "def time_bin_mean(fr_seg, bin_size: int | None):\n",
    "    if bin_size is None or bin_size <= 1:\n",
    "        return fr_seg\n",
    "    L = fr_seg.shape[2]\n",
    "    L2 = (L // bin_size) * bin_size\n",
    "    if L2 <= 0:\n",
    "        return fr_seg\n",
    "    x = fr_seg[:, :, :L2]\n",
    "    x = x.reshape(x.shape[0], x.shape[1], L2 // bin_size, bin_size).mean(axis=3)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _one_neuron_fisher_diag_perm(Xn_all, y, M, n_pos, n_neg, ridge=1e-6, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Xn_all: (n_trials, L)\n",
    "    y: (n_trials,)\n",
    "    M: (n_perm, n_trials) float32\n",
    "    \"\"\"\n",
    "    X = Xn_all.astype(np.float64, copy=False)\n",
    "    X2 = X * X\n",
    "\n",
    "    total_sum = X.sum(axis=0)      # (L,)\n",
    "    total_sum_sq = X2.sum(axis=0)  # (L,)\n",
    "\n",
    "    Xpos = X[y == 1]\n",
    "    Xneg = X[y == 0]\n",
    "    if Xpos.shape[0] < 2 or Xneg.shape[0] < 2:\n",
    "        return 0.0, 1.0\n",
    "\n",
    "    mu_pos = Xpos.mean(axis=0)\n",
    "    mu_neg = Xneg.mean(axis=0)\n",
    "    delta = mu_pos - mu_neg\n",
    "\n",
    "    var_pos = Xpos.var(axis=0, ddof=1)\n",
    "    var_neg = Xneg.var(axis=0, ddof=1)\n",
    "    sw = var_pos + var_neg + ridge\n",
    "    stat_obs = float(np.sum((delta * delta) / (sw + eps)))\n",
    "\n",
    "    # permutation：pos_sum = M @ X => (n_perm, L)\n",
    "    pos_sum = M @ X\n",
    "    pos_sum_sq = M @ X2\n",
    "\n",
    "    neg_sum = total_sum[None, :] - pos_sum\n",
    "    neg_sum_sq = total_sum_sq[None, :] - pos_sum_sq\n",
    "\n",
    "    pos_mean = pos_sum / n_pos\n",
    "    neg_mean = neg_sum / n_neg\n",
    "    delta_p = pos_mean - neg_mean\n",
    "\n",
    "    pos_var = (pos_sum_sq - (pos_sum * pos_sum) / n_pos) / max(n_pos - 1, 1)\n",
    "    neg_var = (neg_sum_sq - (neg_sum * neg_sum) / n_neg) / max(n_neg - 1, 1)\n",
    "    pos_var = np.maximum(pos_var, 0.0)\n",
    "    neg_var = np.maximum(neg_var, 0.0)\n",
    "\n",
    "    sw_p = pos_var + neg_var + ridge\n",
    "    stat_null = np.sum((delta_p * delta_p) / (sw_p + eps), axis=1)\n",
    "\n",
    "    p = (1.0 + np.sum(stat_null >= stat_obs)) / (len(stat_null) + 1.0)\n",
    "    return stat_obs, p\n",
    "\n",
    "\n",
    "def _fisher_vector_stat_lw(X0, X1, ridge=1e-6):\n",
    "    mu0 = X0.mean(axis=0)\n",
    "    mu1 = X1.mean(axis=0)\n",
    "    delta = mu1 - mu0\n",
    "\n",
    "    R0 = X0 - mu0\n",
    "    R1 = X1 - mu1\n",
    "    R = np.vstack([R0, R1])  # pooled residuals\n",
    "\n",
    "    lw = LedoitWolf(assume_centered=True).fit(R)\n",
    "    Sw = lw.covariance_.copy()\n",
    "    L = Sw.shape[0]\n",
    "    Sw.flat[::L + 1] += ridge\n",
    "\n",
    "    z = np.linalg.solve(Sw, delta)\n",
    "    return float(delta @ z)\n",
    "\n",
    "\n",
    "def _one_neuron_fisher_lw_perm(Xn_all, y, perms, ridge=1e-6):\n",
    "    X0 = Xn_all[y == 0]\n",
    "    X1 = Xn_all[y == 1]\n",
    "    if X0.shape[0] < 2 or X1.shape[0] < 2:\n",
    "        return 0.0, 1.0\n",
    "\n",
    "    obs = _fisher_vector_stat_lw(X0, X1, ridge=ridge)\n",
    "\n",
    "    null = np.empty(len(perms), dtype=float)\n",
    "    for i, yp in enumerate(perms):\n",
    "        X0p = Xn_all[yp == 0]\n",
    "        X1p = Xn_all[yp == 1]\n",
    "        null[i] = _fisher_vector_stat_lw(X0p, X1p, ridge=ridge)\n",
    "\n",
    "    p = (1.0 + np.sum(null >= obs)) / (len(null) + 1.0)\n",
    "    return obs, p\n",
    "\n",
    "\n",
    "def count_sig_neurons_fisher_vector(\n",
    "    data_list,\n",
    "    params: Params,\n",
    "    ana_tt,\n",
    "    ana_bt,\n",
    "    *,\n",
    "    extend_cm=10.0,\n",
    "    n_perm=2000,\n",
    "    alpha=0.05,\n",
    "    seed=0,\n",
    "    cov=\"diag\",        # \"diag\" or \"lw\"\n",
    "    ridge=1e-6,\n",
    "    time_bin=5,        # 5~10\n",
    "    n_jobs=8,\n",
    "    backend=\"threading\",\n",
    "):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    out = []\n",
    "\n",
    "    for si, data in enumerate(tqdm(data_list, desc=\"sessions\", dynamic_ncols=True)):\n",
    "        data_AB, data_BA = extract_ab_ba_one_session(data, params, ana_tt, ana_bt, extend_cm=extend_cm)\n",
    "        if data_AB is None:\n",
    "            out.append({\"session\": si, \"n_sig\": 0, \"n_neuron\": 0, \"p\": None, \"q\": None})\n",
    "            continue\n",
    "\n",
    "        data_AB = time_bin_mean(data_AB, time_bin)\n",
    "        data_BA = time_bin_mean(data_BA, time_bin)\n",
    "\n",
    "        n_neuron = data_AB.shape[0]\n",
    "        nAB = data_AB.shape[1]\n",
    "        nBA = data_BA.shape[1]\n",
    "\n",
    "        y = np.concatenate([np.zeros(nAB, int), np.ones(nBA, int)])\n",
    "        n_trials = y.size\n",
    "        n_pos = int(y.sum())\n",
    "        n_neg = int(n_trials - n_pos)\n",
    "        if n_pos < 2 or n_neg < 2:\n",
    "            out.append({\"session\": si, \"n_sig\": 0, \"n_neuron\": n_neuron, \"p\": None, \"q\": None})\n",
    "            continue\n",
    "\n",
    "        perm_idx = np.array([rng.permutation(n_trials) for _ in range(n_perm)], dtype=np.int32)\n",
    "        y_perm = y[perm_idx]\n",
    "        M = (y_perm == 1).astype(np.float32)\n",
    "\n",
    "        perms = [y_perm[i, :] for i in range(n_perm)]\n",
    "\n",
    "        def _task(neuron_id: int):\n",
    "            Xn_all = np.vstack([data_AB[neuron_id, :, :], data_BA[neuron_id, :, :]])  # (n_trials, L)\n",
    "            if cov == \"diag\":\n",
    "                return _one_neuron_fisher_diag_perm(Xn_all, y, M, n_pos, n_neg, ridge=ridge)\n",
    "            elif cov == \"lw\":\n",
    "                return _one_neuron_fisher_lw_perm(Xn_all, y, perms, ridge=ridge)\n",
    "            else:\n",
    "                raise ValueError(\"cov must be 'diag' or 'lw'\")\n",
    "\n",
    "        with tqdm_joblib(tqdm(total=n_neuron, desc=f\"session {si} neurons (fisher-{cov})\",\n",
    "                              leave=False, dynamic_ncols=True)):\n",
    "            results = Parallel(n_jobs=n_jobs, backend=backend)(\n",
    "                delayed(_task)(n) for n in range(n_neuron)\n",
    "            )\n",
    "\n",
    "        stat_obs = np.array([r[0] for r in results], dtype=float)\n",
    "        pvals = np.array([r[1] for r in results], dtype=float)\n",
    "\n",
    "        rej, qvals, _, _ = multipletests(pvals, alpha=alpha, method=\"fdr_bh\")\n",
    "        out.append({\n",
    "            \"session\": si,\n",
    "            \"n_sig\": int(rej.sum()),\n",
    "            \"n_neuron\": n_neuron,\n",
    "            \"p\": pvals,\n",
    "            \"q\": qvals,\n",
    "            \"stat\": stat_obs,\n",
    "            \"sig_mask\": rej,\n",
    "            \"cov\": cov,\n",
    "            \"time_bin\": time_bin,\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# example usage\n",
    "# =========================\n",
    "# data_list = get_data(\"your_entry_dir\")\n",
    "# params = set_params(...)\n",
    "# ana_tt = [\"*\"]; ana_bt = [\"correct\"]  # 示例\n",
    "#\n",
    "# 1) 标量 AUC（推荐先跑这个，最快）\n",
    "# res_auc = count_sig_neurons_auc_dprime_fast(\n",
    "#     data_list, params, ana_tt, ana_bt,\n",
    "#     test_stat=\"auc\", reduce=\"mean\",\n",
    "#     n_perm=5000, alpha=0.05,\n",
    "#     n_jobs=10, backend=\"threading\"\n",
    "# )\n",
    "#\n",
    "# 2) Fisher-vector diag（利用窗口向量但快）\n",
    "# res_fdiag = count_sig_neurons_fisher_vector_fast(\n",
    "#     data_list, params, ana_tt, ana_bt,\n",
    "#     cov=\"diag\", time_bin=5,\n",
    "#     n_perm=2000, alpha=0.05,\n",
    "#     n_jobs=10, backend=\"threading\"\n",
    "# )\n",
    "#\n",
    "# 3) Fisher-vector lw（利用时间协方差，但更慢；建议 time_bin>=5）\n",
    "# res_flw = count_sig_neurons_fisher_vector_fast(\n",
    "#     data_list, params, ana_tt, ana_bt,\n",
    "#     cov=\"lw\", time_bin=8,\n",
    "#     n_perm=1000, alpha=0.05,\n",
    "#     n_jobs=10, backend=\"threading\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d1ca858",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_dir = \"../../data/flexible_shift/\"\n",
    "\n",
    "data_list = get_data(entry_dir)\n",
    "\n",
    "params = set_params(tt_preset=\"basic\",\n",
    "                    bt_preset=\"basic\",\n",
    "                    len_pos_average=10,\n",
    "                    gaussian_sigma=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08a3f24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3d635da72e4f6aa2ee42ef0e763f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sessions:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb2181148a04b5c8fe6440f05ad76ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 0 neurons (fisher-lw):   0%|          | 0/221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d7e3c52124f5585dc6d0f09306239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 1 neurons (fisher-lw):   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37282a08612470987caaeafb0c9daab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 2 neurons (fisher-lw):   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452682ef8c6c49559717582ff4ededf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 3 neurons (fisher-lw):   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46edc8198337486eab0d3dc579caf261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 4 neurons (fisher-lw):   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55fd32098eb442a8ed4a219b1545fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 5 neurons (fisher-lw):   0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_flw1 = count_sig_neurons_fisher_vector(\n",
    "    data_list, params, ana_tt=[\"pattern*\"], ana_bt=[\"*\"],\n",
    "    cov=\"lw\", time_bin=10, extend_cm=20,\n",
    "    n_perm=1000, alpha=0.05,\n",
    "    n_jobs=10, backend=\"threading\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf71caa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca6d233b60e4bb689d629f41ffbc977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sessions:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0683e94b1134cf2a5ef66d014ec0a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 0 neurons (fisher-lw):   0%|          | 0/221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef9ec278be344edaed5ccbb0e62bdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 1 neurons (fisher-lw):   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd65d6b1f5b24a72b642a0afc3e46311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 2 neurons (fisher-lw):   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b56526bf1e2461d81967913f8ce4ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 3 neurons (fisher-lw):   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190c7af0823a4e0588cb3fca7c177a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 4 neurons (fisher-lw):   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8715f34308d44fe92b4f4bbcac6b0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "session 5 neurons (fisher-lw):   0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_flw2 = count_sig_neurons_fisher_vector(\n",
    "    data_list, params, ana_tt=[\"position*\"], ana_bt=[\"*\"],\n",
    "    cov=\"lw\", time_bin=10, extend_cm=20,\n",
    "    n_perm=1000, alpha=0.05,\n",
    "    n_jobs=10, backend=\"threading\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fb3a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9 / 221\n",
      "1 13 / 247\n",
      "2 0 / 119\n",
      "3 0 / 187\n",
      "4 33 / 150\n",
      "5 49 / 321\n"
     ]
    }
   ],
   "source": [
    "for r in res_flw1:\n",
    "    print(r[\"session\"], r[\"n_sig\"], \"/\", r[\"n_neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2987a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7 / 221\n",
      "1 37 / 247\n",
      "2 3 / 119\n",
      "3 18 / 187\n",
      "4 63 / 150\n",
      "5 26 / 321\n"
     ]
    }
   ],
   "source": [
    "for r in res_flw2:\n",
    "    print(r[\"session\"], r[\"n_sig\"], \"/\", r[\"n_neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13612b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
